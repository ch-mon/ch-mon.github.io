<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Projects on Chris Mony</title>
        <link>/projects/</link>
        <description>Recent content in Projects on Chris Mony</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 28 Nov 2020 00:00:00 +0000</lastBuildDate>
        <atom:link href="/projects/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Hack4Good</title>
            <link>/projects/hack4good/</link>
            <pubDate>Sat, 28 Nov 2020 00:00:00 +0000</pubDate>
            
            <guid>/projects/hack4good/</guid>
            <description>Hack4Good Banner. Photo from the H4G website.
  Background During my time at ETH Zurich, my team (big shout-out to Frederike Lübeck, Kaoru Schwarzenegger, and Vincent Bardenhagen) and I had the chance to participate in the two-month-long sustainability Hack4Good program. This hackathon was organized by the Analytics Club and the Student Project House (SPH) at ETH. Five teams would be presented with challenges and data offered by different NGOs and solve the former within the given timeframe.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/Hack4GoodBanner.png"
         alt="Hack4Good Banner. Photo from the H4G website." height="300"/> <figcaption>
            <p>Hack4Good Banner. Photo from the <a href="https://analytics-club.org/hack4good">H4G website</a>.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>During my time at ETH Zurich, my team (big shout-out to <a href="https://www.linkedin.com/in/frederike-l%C3%BCbeck-b3a6b4186/">Frederike Lübeck</a>, <a href="https://www.linkedin.com/in/kaoruschwarzenegger/">Kaoru Schwarzenegger</a>, and <a href="https://www.linkedin.com/in/vincent-bardenhagen-331911115/">Vincent Bardenhagen</a>) and I had the chance to participate in the two-month-long sustainability Hack4Good program. This hackathon was organized by the Analytics Club and the Student Project House (SPH) at ETH. Five teams would be presented with challenges and data offered by different NGOs and solve the former within the given timeframe. Each of the projects had a strong focus on sustainability. Besides the task and various hackdays, Hack4Good also included various workshops, such as about agile project management with Siemens, impact creation with the ETH SPH, or pitching skills with McKinsey.</p>
<h2 id="project-description">Project description</h2>
<blockquote>
<p>Quick note: You can also download the final PDF report <a href="../pdfs/H4G_final_report.pdf">here</a>.</p>
</blockquote>
<p>Our task was provided by the organization &ldquo;Gesellschaft für Internationale Zusammenarbeit&rdquo; (GIZ). Together with the &ldquo;Community Markets for Conservation&rdquo; (COMACO), they are currently active in Eastern Zambia to support people in the agricultural sector. Actually, 52% of the population in Eastern Zambia is employed in agriculture (<a href="http://zm.one.un.org/sites/default/files/un_country_analysis_report.pdf">UN Zambia Report</a>). Nevertheless, every third child has suffered from malnutrition (<a href="http://www.dhsprogram.com/pubs/pdf/FR361/FR361.pdf">Zambia Demographic and Health Survey</a>). Thus, GIZ currently promotes the plantation of two value chain crops in Eastern Zambia: groundnuts and soybeans. Since the growth of these crops is strongly influenced by external factors such as precipitation or soil quality, they were interested in the most important drivers and their impact. For this reason, we started with the goal of setting up a  model to predict crop yield variation in Eastern Zambia.</p>
<p>The pipeline can be visualized in the following figure. As you can see, we dealt with three primary data sources - <em>survey_data</em>, <em>soil data</em>, and <em>weather data</em> - which we describe in the next sections. Lastly, we also explain the methodological aspect of our analysis.</p>
<figure class="center">
    <img src="../images/Hack4GoodPipeline.png"
         alt="Schematic visualization of the end-to-end pipeline."/> <figcaption>
            <p>Schematic visualization of the end-to-end pipeline.</p>
        </figcaption>
</figure>

<h4 id="a-survey-data">a) Survey data</h4>
<p>First, we obtained data from surveys conducted by GIZ in the years 2016-2019. Each sample in this data originated from a survey conducted with one farm in Eastern Zambia. On average, there were about 500 surveys conducted each year. Here, the surveys comprised approximately 600 questions, each of which would map to an individual variable. On the one hand, these variables included the harvested <code>weight</code> and <code>production area</code> for each crop, which then allowed us to derive our target variable <code>land productivity</code>. On the other hand, the surveys contained questions addressing each farm&rsquo;s socioeconomic circumstances, such as <code>household members</code> or access to <code>farming machinery</code>. Sounds splendid, right? Well, that is also what we thought at first glance.</p>
<p>However, as we started digging into the data, we quickly found that the surveys were far from being consistently conducted over the years. In some years, some questions were not asked; other questions would be changed in their potential answer options but still map to the same variable name. For this reason, we manually had to go through each of the four questionnaires, find the minimal set of overlapping questions, and ensure consistent mapping. In the end, we were left with about 50 intersecting (and relevant) features. We also had to drop many samples due to the non-availability of any location information (e.g., closest village name), which could then be mapped to a GPS coordinate. For example, the whole year 2017 showed a lack of such information. In the end, we were left with 430 samples for groundnut yield and 235 samples for soybean yield.</p>
<h4 id="b-soil--data">b) Soil  data</h4>
<p>Second, we retrieved soil quality data from the identified <a href="https://soilgrids.org/">soilgrid.org</a> data source. The data stemmed from a global soil data model with a spatial resolution of 250 meters. Such a model is beneficial in areas where there is only little measurement data or a lack of local models. The data could be accessed online via a WCS API. We leveraged Python to access and download the data for our area of interest. The data contained various soil properties such as <code>sand content</code>, <code>ph value</code>, or <code>nitrogen content</code> - all available for different depths. Since we tried to model the crop yield as objectively as possible, we included all properties at all depths. Finally, we joined the soil data with the survey data via their GPS coordinates.</p>
<h4 id="c-weather-data">c) Weather data</h4>
<p>Third, we accessed weather data in the form of the <a href="https://www.ecmwf.int/en/era5-land">ERA5 land</a> reanalysis. This reanalysis had a spatial resolution of approximately nine kilometers and an hourly temporal resolution. Again, we retrieved the data through a publicly accessible API, which we addressed via Python. We extracted features such as <code>precipitation</code> and <code>temperature</code> while aggregating them to monthly accuracy. In the end, we only kept the months during the growing season from October to March. Furthermore, we engineered more features such as <code>floods</code> (from an excessive amount of precipitation in a short timespan) or <code>heatwaves</code> (via extended periods of high temperatures). Again, we associated the weather data with the surveys via their GPS location.</p>
<h4 id="d-analysis">d) Analysis</h4>
<p>Lastly, we analyzed the dataset by applying several state-of-the-art machine learning algorithms to it. Therefore, used a 70-30 train-test split. Due to the small amount of samples, cross-validation for the hyperparameter optimization was not a viable solution. In the end, we tested XGBoost, CatBoost, Random Forest, Ridge Regression, and Elastic Net models with varying hyperparameters via a random search. The results were evaluated using the R2 score, RMSE, and correlation plots.</p>
<p>In addition, we also dug deeper into the year 2019 in form of a visual analysis since the former showed the most comprehensive set of survey questions. Here, we paid particular attention to analyzing the aspects discussed as valuable during our weekly meetings with GIZ and COMACO. This allowed us to derive insights about certain measures, such as the perceived impact of agricultural knowledge trainings.</p>
<h2 id="results">Results</h2>
<p>The results have proven rather unsatisfactory. Despite of the model we used, the results were only slightly became better than predicting the average (R2 score of 0.14 and 0.08 for groundnuts and soybeans, respectively). After several meetings with GIZ, we identified several potential reasons for this in the data collection process, i.e., the survey data:</p>
<ol>
<li>Data collection in the field is a challenging task. Especially for our target variable, the units of M and A were numerous, and the resulting values are more often rough estimates than precise measurements.</li>
<li>The GPS measurements are too inaccurate. We could only rely on GPS information of the closest village where the survey was conducted, but not of the farms themselves.</li>
<li>The information about the farmer&rsquo;s characteristics and practices retrieved from the surveys contained too few explanatory factors. This might be because the responses to many questions did not show any variation.</li>
</ol>
<p>Nevertheless, we learned that the soil and weather data leveraged during our modeling process had not been accessible by GIZ and COMACO before. Also, they affirmed that this data alone proves to be valuable since it allows them to combine it with expert knowledge to obtain insights. For this reason, in close cooperation with GIZ and COMACO, we shifted the project&rsquo;s focus and provided them with a POC allowing for easy access and inspection of the data. We implemented this in the form of an interactive Tableau dashboard, which you can check out <a href="https://public.tableau.com/profile/chris3097#!/vizhome/GIZ_16056548246950/Story1?publish=yes">here</a>.</p>
<h2 id="learnings">Learnings</h2>
<p>To conclude, the project was an exciting time during which I learned a lot about the messiness of data science. <strong>Machine Learning models are no silver bullet</strong> and can only get as good as the data you supply them with. Thus, in the beginning of each analytics project, <strong>you can never be certain about its outcome</strong>. However, do not hesitate to <strong>shift focus and  adapt iteratively</strong> if the project does not go as originally intended. Finally, I had a great time working together with my team and learning from them about:</p>
<ul>
<li>Programming
<ul>
<li>Python &amp; corresponding packages
<ul>
<li>Pandas, Geopandas, Owslib, Rasterio, Cartopy, Scikit-learn</li>
</ul>
</li>
<li>Git/GitLab workflow and collaboration</li>
</ul>
</li>
<li>Methodological
<ul>
<li>Geospatial analysis</li>
<li>Manual data cleansing in practice</li>
<li>Handling unexpected challenges</li>
<li>Agile project management
<ul>
<li>We worked in weekly Scrum cycles (collaborating via Trello)</li>
</ul>
</li>
<li>Presentation skills</li>
</ul>
</li>
</ul>
<h2 id="code-and-resources">Code and resources</h2>
<p>The code is available in this <a href="https://gitlab.com/analytics-club/hack4good/hack4good-fall-2020/giz">GitLab repository</a> (eventually not freely accessible yet). Again, you can download the final PDF report <a href="../pdfs/H4G_final_report.pdf">here</a>. Also, feel free to check out the final pitch slides <a href="../pdfs/H4G_final_pitch.pdf">here</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Fashion recognition</title>
            <link>/projects/fashionrecognition/</link>
            <pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate>
            
            <guid>/projects/fashionrecognition/</guid>
            <description>Watch Classification. Photo by Eliud Gil Samaniego.
  Background During quarantine, some friends (Pano Herbe and two more) and I decided to tackle an exciting project. It was concerned with training a Convolutional Neural Network (CNN) to recognize certain fashion articles in Instagram pictures. The final goal was to identify fashion objects from Instagram influencers, create potentially traffic-generating keywords, and use them for Google Ads marketing. This service would be especially valuable for data-driven fashion retailers who want to increase their click-through and conversion rate.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/WatchClassification.jpg"
         alt="Watch Classification. Photo by Eliud Gil Samaniego." height="500"/> <figcaption>
            <p>Watch Classification. Photo by <a href="https://unsplash.com/@eliudartphotographer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Eliud Gil Samaniego</a>.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>During quarantine, some friends (<a href="https://www.linkedin.com/in/panagiotis-herbe/">Pano Herbe</a> and two more) and I decided to tackle an exciting project. It was concerned with training a Convolutional Neural Network (CNN) to recognize certain fashion articles in Instagram pictures. The final goal was to identify fashion objects from Instagram influencers, create potentially traffic-generating keywords, and use them for Google Ads marketing. This service would be especially valuable for data-driven fashion retailers who want to increase their click-through and conversion rate.</p>
<h2 id="project-description">Project description</h2>
<p>We started with the <a href="https://storage.googleapis.com/openimages/web/index.html">Google Open Image Database</a> and trained a <a href="https://pjreddie.com/darknet/yolo/">Yolo-V3</a> CNN in <a href="https://colab.research.google.com/notebooks/intro.ipynb">Google Colab</a> to recognize 25 different fashion articles (e.g. dresses, watches, &hellip;) on several thousands of pictures. Afterward, we scraped images from selected Instagram influencer accounts and used the trained CNN for classification of fashion articles in those images. Then, we cropped the detected items and identified the main color via a k-means clustering algorithm. Finally, we were able to build a keyword of the form &ldquo;Influencer name + main color + fashion article&rdquo; (e.g. &ldquo;David Beckham Golden Watch&rdquo;). We also built a small front-end for the CNN classification using <a href="https://www.streamlit.io/">Streamlit</a>. In the next paragraph, I discuss what would happen to the keyword after generation.</p>
<p>The whole project is based upon one assumption: <em>Influencer Instagram posts with fashion articles lead to increased search volume for this article in the consecutive hours/days</em>. First, we tried to verify this thesis manually, however then moved on to apply an automated Google Trends Mapping. We queried the keyword to GoogleTrends via a Python-API (<a href="https://pypi.org/project/pytrends/">PyTrends</a>) and checked for any significant peaks in the search interest. In contrast, in production, one would directly use the obtained keyword for Google Ads. The whole project pipeline is illustrated below.</p>
<figure class="center">
    <img src="../images/FashionAdPipeline.jpg"
         alt="Schematic vizualization of the end-to-end pipeline."/> <figcaption>
            <p>Schematic vizualization of the end-to-end pipeline.</p>
        </figcaption>
</figure>

<h2 id="results">Results</h2>
<p>We brought the project to a point where the pipeline could be executed from one main file. The algorithm would receive a list of Instagram accounts, automatically generate the keywords from the scraped images, and create plots from Google Trends data.</p>
<p>An example of a classified image is shown in the first picture of this post. Here, the CNN detected the watch with 94% confidence. One might wonder why the shoe is not classified as well, since it is also well visible. This results from the fact that our training categories did not involve shoes directly but rather the more specialized forms &ldquo;boot&rdquo;, &ldquo;high heel&rdquo;, and &ldquo;sandal&rdquo;. Since our focus laid on creating a Proof of Concept (PoC), of course, more categories are a natural extension.</p>
<p>However, at this point, we encountered more severe difficulties. First,  Google Trends data has to be interpreted with care since it only gives you a relative number of search queries. Until now, we still had no way of objectively verifying our basic assumption that our keywords would actually generate more traffic. Second, often the CNN would (correctly) recognize items that were almost invisible or not in the main focus of the image. Even a CNN with 100% accuracy would not be able to decide upon the &ldquo;quality&rdquo; of keywords (i.e. keywords from items that receive little to no traffic due to not being in the focus of the picture). Due to these two impediments, which we could not resolve with our resources, we decided to end the project here and take it as a great learning experience.</p>
<h2 id="learnings">Learnings</h2>
<p>To conclude, for me, the project was a fascinating chance for learning more about &ldquo;real&rdquo; data science. We had to dig into different solutions for the arising challenges during development. I was able to strengthen my skills in:</p>
<ul>
<li>Programming
<ul>
<li>Python &amp; corresponding packages
<ul>
<li>Pandas, Selenium, Pillow, PyTrends, Streamlit</li>
</ul>
</li>
<li>Anaconda</li>
<li>Google Colab interaction</li>
</ul>
</li>
<li>Methodological
<ul>
<li>Web scraping</li>
<li>Training of &amp; classifying with state-of-the-art image recognition models (CNNs)</li>
<li>Basic frontend-setup for Data Science projects</li>
</ul>
</li>
</ul>
<h2 id="code">Code</h2>
<p>Code is currently still private (&amp; messy), however, we work on making it public soon.</p>
]]></content>
        </item>
        
        <item>
            <title>Learning phase transitions with Machine Learning</title>
            <link>/projects/phasetransition/</link>
            <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
            
            <guid>/projects/phasetransition/</guid>
            <description>The first principal component shows high correlation with the order parameter.
  Background This project is concerned with the work of my Bachelor&amp;rsquo;s thesis. During the four months, I investigated the phase transition of the Ising model with machine learning and applied the same methodology to another statistical model. Hereby, apart from a practical course during the studies, I had never worked with machine learning before. Thus, this was a great opportunity for me to dig deeper into the topic.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/OrderParameterRodModel.png"
         alt="The first principal component shows high correlation with the order parameter." width="600"/> <figcaption>
            <p>The first principal component shows high correlation with the order parameter.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>This project is concerned with the work of my <strong>Bachelor&rsquo;s thesis</strong>. During the four months, I investigated the phase transition of the Ising model with machine learning and applied the same methodology to another statistical model. Hereby, apart from a practical course during the studies, I had never worked with machine learning before. Thus, this was a great opportunity for me to dig deeper into the topic. Moreover, the thesis formed my first real contact with the Python programming language.</p>
<h2 id="project-description">Project description</h2>
<p>First, we began by reproducing the previous work of other researchers (<a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.96.022140">Wetzel, 2017</a>; <a href="https://arxiv.org/abs/1803.08823">Metha et. al., 2018</a>) and learned the order parameter of the 2D Ising model. Hereby, we relied on two different approaches: <strong>Principal Component Analysis (PCA)</strong> and <strong>Variational Autoencoders (VAEs)</strong>. In the next step, we applied the same methodology to another physical model, the so-called rod-model (<a href="https://aip.scitation.org/doi/abs/10.1063/1.4960618">Oettel et. al., 2016</a>). For both models, the necessary training configurations were generated via <strong>Markov-Chain-Monte-Carlo (MCMC)</strong> sampling.</p>
<h2 id="results">Results</h2>
<p>We were able to show that even for the new model, the machine learning methods can extract the order parameter by themselves (see plot above).</p>
<h2 id="learnings">Learnings</h2>
<p>For me, the thesis provided an excellent opportunity to explore the domain of machine learning. The most important learnings were:</p>
<ul>
<li>Programming
<ul>
<li>Python &amp; corresponding packages
<ul>
<li>Numpy, Matplotlib, Keras, Scikit-Learn</li>
</ul>
</li>
<li>Anaconda</li>
<li>Linux Shell interaction</li>
</ul>
</li>
<li>Methodological
<ul>
<li>Unsupervised machine learning</li>
<li>Principal Component Analysis (PCA)</li>
<li>Variational Autoencoders (VAEs)</li>
<li>Markov-Chain-Monte-Carlo (MCMC) sampling</li>
</ul>
</li>
</ul>
<h2 id="code">Code</h2>
<p>The code for my thesis can be accessed in <a href="https://github.com/ch-mon/Bachelor-thesis">this GitHub repo</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>N.E.S.T. project</title>
            <link>/projects/nestproject/</link>
            <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
            
            <guid>/projects/nestproject/</guid>
            <description>Logo of the nestbau AG. Photo taken from the nestbau website.
  Background During my time with the student consultancy InOne Consult e.V. in Tuebingen, I had the chance to work on several exciting projects. Amongst one of them was developing a Social Media Marketing Strategy for a local real estate investment fund. The N.E.S.T.bau AG (short nestbau) plans, builds, and rents out flats in the Tuebingen region under the guidance of ethical principles.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/NestbauProject.png"
         alt="Logo of the nestbau AG. Photo taken from the nestbau website."/> <figcaption>
            <p>Logo of the nestbau AG. Photo taken from the <a href="https://www.nestbau-ag.de/">nestbau website</a>.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>During my time with the student consultancy InOne Consult e.V. in Tuebingen, I had the chance to work on several exciting projects. Amongst one of them was developing a Social Media Marketing Strategy for a local real estate investment fund. The N.E.S.T.bau AG (short nestbau) plans, builds, and rents out flats in the Tuebingen region under the guidance of ethical principles. Here, N.E.S.T. is an acronym for the German words &ldquo;Sustainable. Ethical. Safe. Transparent.&rdquo;. The CEO realized that he was operating in a saturated market investor-wise and thus approached us to develop a Social Media Marketing Strategy to address younger generations.</p>
<h2 id="project-description">Project description</h2>
<p>We structured the 3.5 month-long project into the four different phases shown below.</p>
<figure class="center">
    <img src="../images/StrategyPipeline.png"
         alt="The framework we followed through the project."/> <figcaption>
            <p>The framework we followed through the project.</p>
        </figcaption>
</figure>

<h4 id="induction-phase">Induction phase</h4>
<p>During the induction phase, we familiarized ourselves with the provided material. We clearly defined our goal using the <strong>SMART method</strong> (i.e., making our goal Specific, Measurable, Attainable, Realistic, and Time-Bound). Furthermore, we used a <strong>project canvas</strong> to become aware of our current position, essential milestones, and available resources.</p>
<h4 id="market-analysis">Market analysis</h4>
<p>The market analysis was subdivided into several aspects.</p>
<ul>
<li>During the <strong>stakeholder analysis</strong>, we highlighted the most relevant players in the market segment and attained actionable measures for each of them.</li>
<li>During the <strong>competitor analysis</strong>, we obtained deep insight into the current field of real estate investment funds and the position of nestbau in it. Quickly, it became apparent that only a few investment funds pay value to social return on investment.</li>
<li>The insights we obtained so far were used during a <strong>SWOT (Strength-Weaknesses-Opportunites-Threads) analysis</strong> to become aware of both strengths and weaknesses and how they potentially can benefit or harm nestbau.</li>
<li>Further, we conducted <strong>legal research</strong> due to regulatory issues regarding the direct advertisement of stocks.</li>
<li>In <strong>interviews with current investors</strong>, we obtained further insights into the motives of investing in nestbau.</li>
<li>In the next step, we channeled much of the before obtained knowledge to identifying the <strong>USP (Unique Selling Point)</strong> of nestbau.</li>
<li>Lastly, we derived so-called <strong>Personas</strong>, which later helped us addressing the hopes and potential fears of the target audience.</li>
</ul>
<h4 id="stragegy-development">Stragegy development</h4>
<p>During the strategy development, we distilled all of the collected knowledge to derive concrete actions on how to best present the identified USP to our personas. Therefore, we worked out concrete methods, such as <strong>They-Ask-You-Answer (TAYA)</strong> or <strong>Search-Engine-Optimization (SEO)</strong>, to generate traffic and engage with the audience. Moreover, we <strong>identified concrete channels and the most suitable content</strong> for each of them. We condensed this knowledge into one of our final deliverables: the <strong>strategy paper</strong>.</p>
<h4 id="strategy-implementation">Strategy implementation</h4>
<blockquote>
<p>A strategy is only as good as its implementation.</p>
</blockquote>
<p>Finally, we also implemented most of the recommended measures. More precisely, this involved <strong>setting up a net of Social Media channels</strong> and linking them amongst each other. Each account would be prefilled with <strong>content for the first weeks</strong>, ensuring a smooth transition to nestbau.</p>
<h2 id="results">Results</h2>
<p>During the project and after the final presentation, we received highly positive feedback on our work from nestbau. In the end, even two additional projects for InOne Consult evolved from this cooperation. After pitching this project to the JCNetwork (a national student consultancy association), we got awarded the best project of the year by PricewaterhouseCoopers (PwC).</p>
<h2 id="learnings">Learnings</h2>
<p>To conclude, for me, the project resembled an excellent opportunity to experience &ldquo;real&rdquo; consulting and applying different frameworks to solve not only theoretical cases. In summary, I learned about</p>
<ul>
<li>Consulting
<ul>
<li>Goal factoring</li>
<li>Strategy development process</li>
<li>Market analysis</li>
</ul>
</li>
<li>Business
<ul>
<li>Social Media Marketing</li>
<li>Search Engine Optimization (SEO)</li>
<li>Agile project management (Scrum)</li>
</ul>
</li>
<li>Soft skills
<ul>
<li>Interdisciplinary team-work</li>
<li>Presentation skills</li>
<li>Communication/writing skills</li>
</ul>
</li>
</ul>
<h2 id="links">Links</h2>
<p>Here you will find feedback from the <a href="https://www.nestbau-ag.de/news/kooperation-mit-der-studentischen-unternehmensberatung-inone-consult-e-v">nestbau blog</a> and more information on the <a href="https://www.jcnetwork.de/projektdesjahres/ehemaligewinner/">JCN website</a>.</p>
]]></content>
        </item>
        
    </channel>
</rss>
