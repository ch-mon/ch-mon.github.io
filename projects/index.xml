<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Projects on Chris Mony</title>
        <link>/projects/</link>
        <description>Recent content in Projects on Chris Mony</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 28 Nov 2020 00:00:00 +0000</lastBuildDate>
        <atom:link href="/projects/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Hack4Good Hackathon</title>
            <link>/projects/hack4good/</link>
            <pubDate>Sat, 28 Nov 2020 00:00:00 +0000</pubDate>
            
            <guid>/projects/hack4good/</guid>
            <description>Watch Classification. Photo by Eliud Gil Samaniego.
  Background During my time at ETH Zurich, I had the chance to participate in the two month-long sustainability Hack4Good program. This hackathon was organized by the Analytics Club and the Student Project House at ETH. Five teams would be presented with challenges and data offered by different NGOs and try to solve the former within the given timeframe. Each of the projects had the focus on sustainability and was pro-bono.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/WatchClassification.jpg"
         alt="Watch Classification. Photo by Eliud Gil Samaniego." height="500"/> <figcaption>
            <p>Watch Classification. Photo by <a href="https://unsplash.com/@eliudartphotographer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Eliud Gil Samaniego</a>.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>During my time at ETH Zurich, I had the chance to participate in the two month-long sustainability Hack4Good program. This hackathon was organized by the Analytics Club and the Student Project House at ETH. Five teams would be presented with challenges and data offered by different NGOs and try to solve the former within the given timeframe. Each of the projects had the focus on sustainability and was pro-bono.</p>
<h2 id="project-description">Project description</h2>
<p>Our task was provided by the NGO &ldquo;Gesellschaft fuer Internatiaonale Zusammenarbeit&rdquo; (GIZ). Currently, they are active in the region of Eastern Zambia to support the people in the agricultural sector. Actually, 52% of the population in Eastern Zambia is employed in agriculture. Nevertheless, every third child has suffered from malnutrition. Thus, GIZ currently promotes the plantation of several value chain crops in Eastern Zambia, amongst which there are groundnuts and soybeans. Since the growth of these crops is strongly influenced by external factors such as precipitation or soil quality, they were interested which the most important factors are and how this relationship looks like. Out of this reson, we started with the goal of setting up a  model to predict crop yield varition in Eastern Zambia.</p>
<p>The pipeline can be vizualized in the following figure. First, we obtained survey data of the years 2016 until 2019 from GIZ. Each sample in this data originated from a survey conducted with one farm in Eastern Zambia. On average, there were about 500 surveys conducted each year. Hereby, the surveys were comprised of about 600 questions each of which would map to a variable. One the one hand, these variables included the harvested <code>weight</code> (in kg) and <code>production area</code> (in ha) for each crop, which then allowed to derive our target variable <code>land productivity</code>. On the other hand, the surveys contained questions addressing the socioeconomic circumstances of each farm such as <code>household members</code> or access to <code>farming machinery</code>. Sounds wonderful, right? Well, that is also what we thought at first. However, as soon as we started digging into the data, we quickly found that the surveys were not consistent over the years. In some years, some questions were not asked, other questions would be changed in their framing or answer options but still map to the same variable name. For this reason, we had to go through each of the questionaires of each year manually, find the minimal set of questions avaliable in all years and ensure consistent mapping. In the end, we were left with about 50 intersecting (and relevant) feautures. As if not enough, we also had to drop many samples due to non avaliablility of any location information (e.g. village name) which could then be mapped to a GPS coordinate. For example, the whole year 2017 showed a lack of such information. In the end, we were left with approximately 500 samples for groundnut yield and 300 samples for soybean yield. That is at least something;)</p>
<p>Second, we retrieved soil quality data from the before as suitable identified soilgrid.org data source. The data stemmed from a global model with a spatial resolution of 250 meters, and is especially helpful in areas where there is only little measurement data or a lack of local models. The data could be accessed online via a WCS API. We simply could leverage Python to access and download the data in our area of interest. The data showed various properties of soil such as <code>sand content</code>, <code>ph-value</code> or <code>nitrogen content</code>, all available for different depths. Since we tried to model the crop yield as objectively as possible, we included all variables at all depths. We mapped them to each GPS location of a survey by taking the average of the surrounding three kilometers. That way we added about 60 new features to our final dataset.</p>
<p>Third, we accessed weather data in form of the ERA5 land reanalysis. The reanalysis had a spatial resolution of approximately nine kilometers and an hourly temporal resolution. Again, the access was in form of a publicly accessibale API which we adressed via Python. We extracted features as precipitation and temperatures, while aggregating them to monthly accuracy. In the end, we only kept the months during the growing season from October to March. Furthermore, we engineered some feautres such as <code>floods</code> (from an excessive amount pf precipitation in a short timespan) or <code>heatwaves</code> (via extended periods of high temperatures.). We mapped the weather data to each GPS location by taking the average over the surrounding 10 kilometers. In total, we added 100 new features to our final dataset.</p>
<p>Lastely, we analyzed the dataset by applying several state-of-the-art machine learning algorithms to it. Therefore, used a 70-30 train-test split. Due to the small sample amount, cross-validation for hyperparameter selection was not a viable solution. In the end, we  tested XGBoost, CatBoost, Random Forest, Ridge Regression, and Elastic Net models with varying hyperparameters. The results were evaluated using the R2 score, RMSE, and correlation plots.</p>
<p>Also, even though not being in the direct focus of our project we also dug deeper into the most comprehesive survey year 2019. Here we payed special attention to aspects of interest which we obtained from our weekly meetings. This allowed us to derive insights such as the percieved impact of financial gifts to the farmers.</p>
<figure class="center">
    <img src="../images/FashionAdPipeline.jpg"
         alt="Schematic vizualization of the end-to-end pipeline."/> <figcaption>
            <p>Schematic vizualization of the end-to-end pipeline.</p>
        </figcaption>
</figure>

<h2 id="results">Results</h2>
<p>The results have shown to be rather unsatisfactory. Despite which model we used, our models only slightly became better than predicting the average. After several correspondents with the GIZ, we identified the potential reason for this in the data collection process, i.e. the survey conduction.</p>
<ul>
<li>First,</li>
<li>Second,</li>
</ul>
<p>Nevertheless, we also learned that the soil and weather data which we used during our modelling process, has not been used by GIZ before. Also, they found this data alone already to be valuable, since it allowed them to combine it with their expert knowledge to obtain insights into the general situtation. Out of this reason, we in close cooporation with GIZ we shifted the focus of the project and provided them a way in which they could easily access the data. In the end, we implemented this in form of a Tableau dashboard, which you can checkout <a href="">here</a>.</p>
<h2 id="learnings">Learnings</h2>
<p>To conclude, for me, the project was a exciting time during which I learned a lot about the messiness of data science. Machine Learningn models are no silver bullet and can only get as good as the data which you supply them with. Thus, in the beginning you can never be certain what the outcome of a project will be. However, do not hesitate to shift focus and iteratevely adapt if the projects does not go as originally intended. Nevertheless, I had a great time working together with my team and learning from them:</p>
<ul>
<li>Programming
<ul>
<li>Python &amp; corresponding packages
<ul>
<li>Pandas, Geopandas, Owslib, Rasterio, Cartopy, Scikit-learn</li>
</ul>
</li>
<li>Git/GitLab workflow and collaboration</li>
</ul>
</li>
<li>Methodological
<ul>
<li>Geospatial analysis</li>
<li>Manual data cleansing in practice</li>
<li>Dealing with unexpected challenges</li>
<li>Agile project management
<ul>
<li>We worked in Scrum cycles</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="code">Code</h2>
<p>Code is available in this <a href="">GitLab repository</a>. You can read the final report <a href="../pdfs/final_report.pdf">here</a></p>
]]></content>
        </item>
        
        <item>
            <title>Fashion recognition</title>
            <link>/projects/fashionrecognition/</link>
            <pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate>
            
            <guid>/projects/fashionrecognition/</guid>
            <description>Watch Classification. Photo by Eliud Gil Samaniego.
  Background During quarantine, some friends (Pano Herbe and two more) and I decided to tackle an exciting project. It was concerned with training a Convolutional Neural Network (CNN) to recognize certain fashion articles in Instagram pictures. The final goal was to identify fashion objects from Instagram influencers, create potentially traffic-generating keywords, and use them for Google Ads marketing. This service would be especially valuable for data-driven fashion retailers who want to increase their click-through and conversion rate.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/WatchClassification.jpg"
         alt="Watch Classification. Photo by Eliud Gil Samaniego." height="500"/> <figcaption>
            <p>Watch Classification. Photo by <a href="https://unsplash.com/@eliudartphotographer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Eliud Gil Samaniego</a>.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>During quarantine, some friends (<a href="https://www.linkedin.com/in/panagiotis-herbe/">Pano Herbe</a> and two more) and I decided to tackle an exciting project. It was concerned with training a Convolutional Neural Network (CNN) to recognize certain fashion articles in Instagram pictures. The final goal was to identify fashion objects from Instagram influencers, create potentially traffic-generating keywords, and use them for Google Ads marketing. This service would be especially valuable for data-driven fashion retailers who want to increase their click-through and conversion rate.</p>
<h2 id="project-description">Project description</h2>
<p>We started with the <a href="https://storage.googleapis.com/openimages/web/index.html">Google Open Image Database</a> and trained a <a href="https://pjreddie.com/darknet/yolo/">Yolo-V3</a> CNN in <a href="https://colab.research.google.com/notebooks/intro.ipynb">Google Colab</a> to recognize 25 different fashion articles (e.g. dresses, watches, &hellip;) on several thousands of pictures. Afterward, we scraped images from selected Instagram influencer accounts and used the trained CNN for classification of fashion articles in those images. Then, we cropped the detected items and identified the main color via a k-means clustering algorithm. Finally, we were able to build a keyword of the form &ldquo;Influencer name + main color + fashion article&rdquo; (e.g. &ldquo;David Beckham Golden Watch&rdquo;). We also built a small front-end for the CNN classification using <a href="https://www.streamlit.io/">Streamlit</a>. In the next paragraph, I discuss what would happen to the keyword after generation.</p>
<p>The whole project is based upon one assumption: <em>Influencer Instagram posts with fashion articles lead to increased search volume for this article in the consecutive hours/days</em>. First, we tried to verify this thesis manually, however then moved on to apply an automated Google Trends Mapping. We queried the keyword to GoogleTrends via a Python-API (<a href="https://pypi.org/project/pytrends/">PyTrends</a>) and checked for any significant peaks in the search interest. In contrast, in production, one would directly use the obtained keyword for Google Ads. The whole project pipeline is illustrated below.</p>
<figure class="center">
    <img src="../images/FashionAdPipeline.jpg"
         alt="Schematic vizualization of the end-to-end pipeline."/> <figcaption>
            <p>Schematic vizualization of the end-to-end pipeline.</p>
        </figcaption>
</figure>

<h2 id="results">Results</h2>
<p>We brought the project to a point where the pipeline could be executed from one main file. The algorithm would receive a list of Instagram accounts, automatically generate the keywords from the scraped images, and create plots from Google Trends data.</p>
<p>An example of a classified image is shown in the first picture of this post. Here, the CNN detected the watch with 94% confidence. One might wonder why the shoe is not classified as well, since it is also well visible. This results from the fact that our training categories did not involve shoes directly but rather the more specialized forms &ldquo;boot&rdquo;, &ldquo;high heel&rdquo;, and &ldquo;sandal&rdquo;. Since our focus laid on creating a Proof of Concept (PoC), of course, more categories are a natural extension.</p>
<p>However, at this point, we encountered more severe difficulties. First,  Google Trends data has to be interpreted with care since it only gives you a relative number of search queries. Until now, we still had no way of objectively verifying our basic assumption that our keywords would actually generate more traffic. Second, often the CNN would (correctly) recognize items that were almost invisible or not in the main focus of the image. Even a CNN with 100% accuracy would not be able to decide upon the &ldquo;quality&rdquo; of keywords (i.e. keywords from items that receive little to no traffic due to not being in the focus of the picture). Due to these two impediments, which we could not resolve with our resources, we decided to end the project here and take it as a great learning experience.</p>
<h2 id="learnings">Learnings</h2>
<p>To conclude, for me, the project was a fascinating chance for learning more about &ldquo;real&rdquo; data science. We had to dig into different solutions for the arising challenges during development. I was able to strengthen my skills in:</p>
<ul>
<li>Programming
<ul>
<li>Python &amp; corresponding packages
<ul>
<li>Pandas, Selenium, Pillow, PyTrends, Streamlit</li>
</ul>
</li>
<li>Anaconda</li>
<li>Google Colab interaction</li>
</ul>
</li>
<li>Methodological
<ul>
<li>Web scraping</li>
<li>Training of &amp; classifying with state-of-the-art image recognition models (CNNs)</li>
<li>Basic frontend-setup for Data Science projects</li>
</ul>
</li>
</ul>
<h2 id="code">Code</h2>
<p>Code is currently still private (&amp; messy), however, we work on making it public soon.</p>
]]></content>
        </item>
        
        <item>
            <title>Learning phase transitions with Machine Learning</title>
            <link>/projects/phasetransition/</link>
            <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
            
            <guid>/projects/phasetransition/</guid>
            <description>The first principal component shows high correlation with the order parameter.
  Background This project is concerned with the work of my Bachelor&amp;rsquo;s thesis. During the four months, I investigated the phase transition of the Ising model with machine learning and applied the same methodology to another statistical model. Hereby, apart from a practical course during the studies, I had never worked with machine learning before. Thus, this was a great opportunity for me to dig deeper into the topic.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/OrderParameterRodModel.png"
         alt="The first principal component shows high correlation with the order parameter." width="600"/> <figcaption>
            <p>The first principal component shows high correlation with the order parameter.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>This project is concerned with the work of my <strong>Bachelor&rsquo;s thesis</strong>. During the four months, I investigated the phase transition of the Ising model with machine learning and applied the same methodology to another statistical model. Hereby, apart from a practical course during the studies, I had never worked with machine learning before. Thus, this was a great opportunity for me to dig deeper into the topic. Moreover, the thesis formed my first real contact with the Python programming language.</p>
<h2 id="project-description">Project description</h2>
<p>First, we began by reproducing the previous work of other researchers (<a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.96.022140">Wetzel, 2017</a>; <a href="https://arxiv.org/abs/1803.08823">Metha et. al., 2018</a>) and learned the order parameter of the 2D Ising model. Hereby, we relied on two different approaches: <strong>Principal Component Analysis (PCA)</strong> and <strong>Variational Autoencoders (VAEs)</strong>. In the next step, we applied the same methodology to another physical model, the so-called rod-model (<a href="https://aip.scitation.org/doi/abs/10.1063/1.4960618">Oettel et. al., 2016</a>). For both models, the necessary training configurations were generated via <strong>Markov-Chain-Monte-Carlo (MCMC)</strong> sampling.</p>
<h2 id="results">Results</h2>
<p>We were able to show that even for the new model, the machine learning methods can extract the order parameter by themselves (see plot above).</p>
<h2 id="learnings">Learnings</h2>
<p>For me, the thesis provided an excellent opportunity to explore the domain of machine learning. The most important learnings were:</p>
<ul>
<li>Programming
<ul>
<li>Python &amp; corresponding packages
<ul>
<li>Numpy, Matplotlib, Keras, Scikit-Learn</li>
</ul>
</li>
<li>Anaconda</li>
<li>Linux Shell interaction</li>
</ul>
</li>
<li>Methodological
<ul>
<li>Unsupervised machine learning</li>
<li>Principal Component Analysis (PCA)</li>
<li>Variational Autoencoders (VAEs)</li>
<li>Markov-Chain-Monte-Carlo (MCMC) sampling</li>
</ul>
</li>
</ul>
<h2 id="code">Code</h2>
<p>The code for my thesis can be accessed in <a href="https://github.com/ch-mon/Bachelor-thesis">this GitHub repo</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>N.E.S.T. project</title>
            <link>/projects/nestproject/</link>
            <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
            
            <guid>/projects/nestproject/</guid>
            <description>Logo of the nestbau AG. Photo taken from the nestbau website.
  Background During my time with the student consultancy InOne Consult e.V. in Tuebingen, I had the chance to work on several exciting projects. Amongst one of them was developing a Social Media Marketing Strategy for a local real estate investment fund. The N.E.S.T.bau AG (short nestbau) plans, builds, and rents out flats in the Tuebingen region under the guidance of ethical principles.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="../images/NestbauProject.png"
         alt="Logo of the nestbau AG. Photo taken from the nestbau website."/> <figcaption>
            <p>Logo of the nestbau AG. Photo taken from the <a href="https://www.nestbau-ag.de/">nestbau website</a>.</p>
        </figcaption>
</figure>

<h2 id="background">Background</h2>
<p>During my time with the student consultancy InOne Consult e.V. in Tuebingen, I had the chance to work on several exciting projects. Amongst one of them was developing a Social Media Marketing Strategy for a local real estate investment fund. The N.E.S.T.bau AG (short nestbau) plans, builds, and rents out flats in the Tuebingen region under the guidance of ethical principles. Here, N.E.S.T. is an acronym for the German words &ldquo;Sustainable. Ethical. Safe. Transparent.&rdquo;. The CEO realized that he was operating in a saturated market investor-wise and thus approached us to develop a Social Media Marketing Strategy to address younger generations.</p>
<h2 id="project-description">Project description</h2>
<p>We structured the 3.5 month-long project into the four different phases shown below.</p>
<figure class="center">
    <img src="../images/StrategyPipeline.png"
         alt="The framework we followed through the project."/> <figcaption>
            <p>The framework we followed through the project.</p>
        </figcaption>
</figure>

<h4 id="induction-phase">Induction phase</h4>
<p>During the induction phase, we familiarized ourselves with the provided material. We clearly defined our goal using the <strong>SMART method</strong> (i.e., making our goal Specific, Measurable, Attainable, Realistic, and Time-Bound). Furthermore, we used a <strong>project canvas</strong> to become aware of our current position, essential milestones, and available resources.</p>
<h4 id="market-analysis">Market analysis</h4>
<p>The market analysis was subdivided into several aspects.</p>
<ul>
<li>During the <strong>stakeholder analysis</strong>, we highlighted the most relevant players in the market segment and attained actionable measures for each of them.</li>
<li>During the <strong>competitor analysis</strong>, we obtained deep insight into the current field of real estate investment funds and the position of nestbau in it. Quickly, it became apparent that only a few investment funds pay value to social return on investment.</li>
<li>The insights we obtained so far were used during a <strong>SWOT (Strength-Weaknesses-Opportunites-Threads) analysis</strong> to become aware of both strengths and weaknesses and how they potentially can benefit or harm nestbau.</li>
<li>Further, we conducted <strong>legal research</strong> due to regulatory issues regarding the direct advertisement of stocks.</li>
<li>In <strong>interviews with current investors</strong>, we obtained further insights into the motives of investing in nestbau.</li>
<li>In the next step, we channeled much of the before obtained knowledge to identifying the <strong>USP (Unique Selling Point)</strong> of nestbau.</li>
<li>Lastly, we derived so-called <strong>Personas</strong>, which later helped us addressing the hopes and potential fears of the target audience.</li>
</ul>
<h4 id="stragegy-development">Stragegy development</h4>
<p>During the strategy development, we distilled all of the collected knowledge to derive concrete actions on how to best present the identified USP to our personas. Therefore, we worked out concrete methods, such as <strong>They-Ask-You-Answer (TAYA)</strong> or <strong>Search-Engine-Optimization (SEO)</strong>, to generate traffic and engage with the audience. Moreover, we <strong>identified concrete channels and the most suitable content</strong> for each of them. We condensed this knowledge into one of our final deliverables: the <strong>strategy paper</strong>.</p>
<h4 id="strategy-implementation">Strategy implementation</h4>
<blockquote>
<p>A strategy is only as good as its implementation.</p>
</blockquote>
<p>Finally, we also implemented most of the recommended measures. More precisely, this involved <strong>setting up a net of Social Media channels</strong> and linking them amongst each other. Each account would be prefilled with <strong>content for the first weeks</strong>, ensuring a smooth transition to nestbau.</p>
<h2 id="results">Results</h2>
<p>During the project and after the final presentation, we received highly positive feedback on our work from nestbau. In the end, even two additional projects for InOne Consult evolved from this cooperation. After pitching this project to the JCNetwork (a national student consultancy association), we got awarded the best project of the year by PricewaterhouseCoopers (PwC).</p>
<h2 id="learnings">Learnings</h2>
<p>To conclude, for me, the project resembled an excellent opportunity to experience &ldquo;real&rdquo; consulting and applying different frameworks to solve not only theoretical cases. In summary, I learned about</p>
<ul>
<li>Consulting
<ul>
<li>Goal factoring</li>
<li>Strategy development process</li>
<li>Market analysis</li>
</ul>
</li>
<li>Business
<ul>
<li>Social Media Marketing</li>
<li>Search Engine Optimization (SEO)</li>
<li>Agile project management (Scrum)</li>
</ul>
</li>
<li>Soft skills
<ul>
<li>Interdisciplinary team-work</li>
<li>Presentation skills</li>
<li>Communication/writing skills</li>
</ul>
</li>
</ul>
<h2 id="links">Links</h2>
<p>Here you will find feedback from the <a href="https://www.nestbau-ag.de/news/kooperation-mit-der-studentischen-unternehmensberatung-inone-consult-e-v">nestbau blog</a> and more information on the <a href="https://www.jcnetwork.de/projektdesjahres/ehemaligewinner/">JCN website</a>.</p>
]]></content>
        </item>
        
    </channel>
</rss>
